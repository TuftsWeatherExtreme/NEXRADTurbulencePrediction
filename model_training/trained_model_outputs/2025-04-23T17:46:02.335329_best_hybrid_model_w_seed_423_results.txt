[Epoch: 1, Batch_num:   100] avg training loss per batch: 3.041[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.714[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.712[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.725[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.718[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.709[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.705[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.710[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.705[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.703[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.709[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.708[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.706[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.711[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.710[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.725[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.810[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.734[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.725[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.728[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.730[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.722[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.795[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.729[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.729[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.735[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.728[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.735[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.730[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.725[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.778[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.912[Epoch: 4, Batch_num:   600] avg training loss per batch: 2.558[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.748[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.740[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.760[Epoch: 1, Batch_num:   100] avg training loss per batch: 3.856[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.716[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.708[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.708[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.709[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.712[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.709[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.710[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.709[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.707[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.719[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.709[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.710[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.707[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.705[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.704[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.709[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.704[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.705[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.704[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.703[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.704[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.703[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.702[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.722[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.700[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.700[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.702[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.703[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.702[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.703[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.702[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.700[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.701[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.701[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.704[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.295[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.707[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.705[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.708[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.705[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.713[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.704[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.708[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.705[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.706[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.703[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.707[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.705[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.709[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.710[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.704[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.704[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.705[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.706[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.706[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.705[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.736[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.711[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.711[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.706[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.705[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.703[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.703[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.703[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.703[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.719[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.704[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.704[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.703[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.721[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.707[Epoch: 1, Batch_num:   100] avg training loss per batch: 14.501[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.615[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.609[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.608[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.607[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.612[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.608[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.610[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.611[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.610[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.608[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.608[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.611[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.610[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.609[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.609[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.608[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.608[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.606[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.606[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.610[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.608[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.606[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.604[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.606[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.608[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.607[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.635[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.607[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.605[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.605[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.604[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.606[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.604[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.606[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.604[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.903[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.731[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.738[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.734[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.716[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.704[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.702[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.704[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.705[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.702[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.701[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.701[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.701[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.703[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.707[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.706[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.702[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.704[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.704[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.701[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.698[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.704[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.702[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.701[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.705[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.703[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.704[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.703[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.704[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.705[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.701[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.700[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.713[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.705[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.704[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.734[Epoch: 1, Batch_num:   100] avg training loss per batch: 5.242[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.712[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.707[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.705[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.704[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.699[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.699[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.701[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.700[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.703[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.700[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.699[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.701[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.701[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.702[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.701[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.703[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.701[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.699[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.702[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.700[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.701[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.701[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.701[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.707[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.702[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.699[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.713[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.702[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.704[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.702[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.699[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.698[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.702[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.700[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.698[Epoch: 1, Batch_num:   100] avg training loss per batch: 10.841[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.704[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.692[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.691[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.680[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.684[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.673[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.677[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.674[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.671[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.677[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.676[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.681[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.669[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.669[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.668[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.668[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.666[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.667[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.671[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.664[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.666[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.673[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.668[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.666[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.662[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.660[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.661[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.660[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.664[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.660[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.662[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.667[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.661[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.660[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.667[Epoch: 1, Batch_num:   100] avg training loss per batch: 15.266[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.680[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.674[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.670[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.675[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.669[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.666[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.665[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.670[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.667[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.660[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.659[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.662[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.658[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.661[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.660[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.658[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.659[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.664[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.660[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.660[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.659[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.660[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.662[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.663[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.662[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.658[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.657[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.657[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.656[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.673[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.662[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.658[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.686[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.687[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.671[Epoch: 1, Batch_num:   100] avg training loss per batch: 1.929[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.611[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.603[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.598[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.594[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.592[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.590[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.591[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.589[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.590[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.589[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.590[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.591[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.598[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.592[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.590[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.589[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.591[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.590[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.589[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.591[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.592[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.592[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.593[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.591[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.589[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.613[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.594[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.592[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.587[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.607[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.595[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.594[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.598[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.592[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.592[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.369[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.687[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.680[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.833[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.673[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.672[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.670[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.664[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.663[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.680[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.668[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.664[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.662[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.662[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.662[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.656[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.657[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.671[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.663[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.660[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.660[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.659[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.663[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.671[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.677[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.674[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.660[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.665[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.662[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.665[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.663[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.673[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.658[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.660[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.662[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.659[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.572[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.708[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.690[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.688[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.674[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.672[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.670[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.663[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.665[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.665[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.660[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.661[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.664[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.659[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.657[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.662[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.664[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.654[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.656[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.656[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.656[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.669[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.661[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.657[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.657[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.652[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.660[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.653[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.653[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.883[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.893[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.722[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.692[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.687[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.688[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.680[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.619[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.692[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.677[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.669[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.668[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.669[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.663[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.662[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.662[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.662[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.661[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.660[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.657[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.657[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.659[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.658[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.656[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.656[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.656[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.658[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.655[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.656[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.656[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.662[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.656[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.658[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.656[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.658[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.659[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.657[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.654[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.655[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.653[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.651[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.656[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.652[Epoch: 1, Batch_num:   100] avg training loss per batch: 6.894[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.668[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.658[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.653[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.650[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.647[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.643[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.644[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.646[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.644[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.640[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.643[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.642[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.642[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.641[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.638[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.639[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.650[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.640[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.641[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.639[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.644[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.647[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.654[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.650[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.642[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.641[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.638[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.642[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.656[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.658[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.642[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.638[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.640[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.640[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.638[Epoch: 1, Batch_num:   100] avg training loss per batch: 3.202[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.665[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.654[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.650[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.643[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.635[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.638[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.640[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.634[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.630[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.633[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.630[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.633[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.633[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.630[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.632[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.631[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.631[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.629[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.627[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.631[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.631[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.630[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.625[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.628[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.630[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.628[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.627[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.625[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.626[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.628[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.627[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.629[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.628[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.627[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.628[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.243[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.710[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.680[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.667[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.656[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.657[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.647[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.645[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.641[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.642[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.637[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.639[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.635[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.639[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.635[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.634[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.634[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.633[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.631[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.633[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.631[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.635[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.630[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.633[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.637[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.641[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.692[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.658[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.641[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.635[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.634[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.633[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.631[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.642[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.649[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.643[Epoch: 1, Batch_num:   100] avg training loss per batch: 11.270[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.680[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.606[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.598[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.594[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.594[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.597[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.590[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.591[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.593[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.592[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.606[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.589[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.592[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.587[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.588[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.586[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.586[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.580[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.583[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.587[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.587[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.587[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.583[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.584[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.583[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.584[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.583[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.583[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.583[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.584[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.586[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.581[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.584[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.585[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.582[Epoch: 1, Batch_num:   100] avg training loss per batch: 3.805[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.683[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.667[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.664[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.660[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.657[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.649[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.653[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.646[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.644[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.643[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.643[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.641[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.643[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.637[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.637[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.638[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.636[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.635[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.635[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.638[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.643[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.638[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.632[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.634[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.635[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.633[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.630[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.630[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.631[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.628[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.628[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.630[Epoch: 1, Batch_num:   100] avg training loss per batch: 12.286[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.678[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.665[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.659[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.655[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.652[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.650[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.651[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.647[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.645[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.642[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.644[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.640[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.638[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.639[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.639[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.637[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.636[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.637[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.633[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.631[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.635[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.631[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.633[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.632[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.633[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.634[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.633[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.629[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.632[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.630[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.631[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.631[Epoch: 1, Batch_num:   100] avg training loss per batch: 12.207[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.690[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.674[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.663[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.659[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.652[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.653[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.646[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.648[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.649[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.643[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.641[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.642[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.639[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.637[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.639[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.645[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.648[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.647[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.641[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.640[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.654[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.658[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.654[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.665[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.663[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.662[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.650[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.644[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.643[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.642[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.638[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.638[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.637[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.635[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.637[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.359[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.694[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.679[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.669[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.666[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.657[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.654[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.653[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.651[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.651[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.651[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.647[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.639[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.636[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.637[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.638[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.636[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.635[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.637[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.636[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.630[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.626[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.627[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.629[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.630[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.630[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.631[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.632[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.630[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.627[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.629[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.630[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.670[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.669[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.661[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.797[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.691[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.674[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.662[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.660[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.662[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.662[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.674[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.667[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.660[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.679[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.664[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.660[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.656[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.652[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.652[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.688[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.680[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.704[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.714[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.714[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.712[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.709[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.712[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.714[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.711[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.709[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.709[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.702[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.698[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.695[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.692[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.692[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.687[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.682[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.675[Epoch: 1, Batch_num:   100] avg training loss per batch: 8.132[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.702[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.682[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.675[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.670[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.669[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.659[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.655[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.665[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.675[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.666[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.663[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.657[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.653[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.663[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.648[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.652[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.654[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.647[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.645[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.654[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.675[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.666[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.664[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.655[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.651[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.654[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.647[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.644[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.646[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.648[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.645[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.645[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.643[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.644[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.665[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.437[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.716[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.717[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.716[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.717[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.715[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.716[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.716[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.716[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.715[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.717[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.717[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.717[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.716[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.714[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.716[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.717[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.715[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.715[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.718[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.717[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.715[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.717[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.715[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.717[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.715[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.715[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.716[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.715[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.716[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.716[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.718[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.715[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.714[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.715[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.716[Epoch: 1, Batch_num:   100] avg training loss per batch: 2.299[Epoch: 1, Batch_num:   200] avg training loss per batch: 1.678[Epoch: 1, Batch_num:   300] avg training loss per batch: 1.671[Epoch: 1, Batch_num:   400] avg training loss per batch: 1.663[Epoch: 1, Batch_num:   500] avg training loss per batch: 1.659[Epoch: 1, Batch_num:   600] avg training loss per batch: 1.652[Epoch: 1, Batch_num:   700] avg training loss per batch: 1.649[Epoch: 1, Batch_num:   800] avg training loss per batch: 1.650[Epoch: 1, Batch_num:   900] avg training loss per batch: 1.644[Epoch: 2, Batch_num:   100] avg training loss per batch: 1.643[Epoch: 2, Batch_num:   200] avg training loss per batch: 1.643[Epoch: 2, Batch_num:   300] avg training loss per batch: 1.645[Epoch: 2, Batch_num:   400] avg training loss per batch: 1.641[Epoch: 2, Batch_num:   500] avg training loss per batch: 1.641[Epoch: 2, Batch_num:   600] avg training loss per batch: 1.640[Epoch: 2, Batch_num:   700] avg training loss per batch: 1.640[Epoch: 2, Batch_num:   800] avg training loss per batch: 1.640[Epoch: 2, Batch_num:   900] avg training loss per batch: 1.639[Epoch: 3, Batch_num:   100] avg training loss per batch: 1.636[Epoch: 3, Batch_num:   200] avg training loss per batch: 1.637[Epoch: 3, Batch_num:   300] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   400] avg training loss per batch: 1.636[Epoch: 3, Batch_num:   500] avg training loss per batch: 1.631[Epoch: 3, Batch_num:   600] avg training loss per batch: 1.636[Epoch: 3, Batch_num:   700] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   800] avg training loss per batch: 1.634[Epoch: 3, Batch_num:   900] avg training loss per batch: 1.650[Epoch: 4, Batch_num:   100] avg training loss per batch: 1.663[Epoch: 4, Batch_num:   200] avg training loss per batch: 1.650[Epoch: 4, Batch_num:   300] avg training loss per batch: 1.642[Epoch: 4, Batch_num:   400] avg training loss per batch: 1.638[Epoch: 4, Batch_num:   500] avg training loss per batch: 1.638[Epoch: 4, Batch_num:   600] avg training loss per batch: 1.636[Epoch: 4, Batch_num:   700] avg training loss per batch: 1.636[Epoch: 4, Batch_num:   800] avg training loss per batch: 1.633[Epoch: 4, Batch_num:   900] avg training loss per batch: 1.645